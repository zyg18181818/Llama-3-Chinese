# Llama-3-Chinese

**ğŸ€„ğŸ‡¨ğŸ‡³ä¸­æ–‡** | [**ğŸ”¤English**](./README_EN.md) 

## âš—ï¸ ä¸»è¦å†…å®¹
- ğŸ’¡æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª**å®Œæ•´çš„å·¥ä½œæµ**ï¼ŒåŒ…æ‹¬å¢é‡é¢„è®­ç»ƒï¼Œå¾®è°ƒï¼ˆå…¨å‚å¾®è°ƒä»¥åŠloraå¾®è°ƒï¼‰ï¼Œå¯¹é½ï¼Œè¯„ä¼°ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªæ‹¥æœ‰å¼ºå¤§ä¸­æ–‡èƒ½åŠ›çš„Llamaæ¨¡å‹
- ğŸ’¡å¼€æºäº†ä½¿ç”¨ä¸­æ–‡æ•°æ®é¢„è®­ç»ƒçš„Llamaä»¥åŠç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„**æ¨¡å‹**
- ğŸ’¡å¼€æºäº†æ‰€ä½¿ç”¨çš„æ‰€æœ‰**æ•°æ®é›†**ï¼Œå¹¶æä¾›äº†æ•°æ®ç­›é€‰æ–¹å¼
- ğŸ’¡å¼€æºäº†æ‰€æœ‰**è®­ç»ƒè„šæœ¬**ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªèº«éœ€æ±‚è¿›ä¸€æ­¥è®­ç»ƒ
- ğŸ’¡æä¾›äº†å„é˜¶æ®µå¾®è°ƒè®¾ç½®å‚æ•°å¯¹åº”**æ—¶é—´æ¶ˆè€—**ä»¥åŠ**æ˜¾å­˜å ç”¨**ï¼Œå¹¶åˆ—å‡ºå½±å“æ˜¾å­˜ä»¥åŠè®­ç»ƒæ—¶é—´çš„å‚æ•°

## ğŸ—‚ï¸ ç›®å½•
- [ğŸ†™ æ›´æ–°](#-æ›´æ–°)
- [âš¡ï¸ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [ğŸš€ å·¥ä½œæµ](#-å·¥ä½œæµ)
- [ğŸ¤— æ¨¡å‹](#-æ¨¡å‹)
   * [ğŸ¤– Llama-3æ¨¡å‹](#-Llama-3æ¨¡å‹)
   * [ğŸ¤– ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹](#-ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹)
   * [ğŸ¤– ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#-ä¸­æ–‡å¾®è°ƒæ¨¡å‹)
   * [ğŸ¤– ä¸­æ–‡å¯¹é½æ¨¡å‹](#-ä¸­æ–‡å¯¹é½æ¨¡å‹)
- [ğŸ“ æ•°æ®é›†](#-æ•°æ®é›†)
   * [ğŸ“„ é¢„è®­ç»ƒæ•°æ®é›†](#-é¢„è®­ç»ƒæ•°æ®é›†)
   * [ğŸ“„ å¾®è°ƒæ•°æ®é›†](#-å¾®è°ƒæ•°æ®é›†)
   * [ğŸ“„ å¯¹é½æ•°æ®é›†](#-å¯¹é½æ•°æ®é›†)
- [ğŸ”§ è®¾ç½®](#-è®¾ç½®)
- [ğŸ“– æ ·ä¾‹](#-æ ·ä¾‹)

## ğŸ†™ æ›´æ–°
- ğŸ”¥ æˆ‘ä»¬åœ¨huggingfaceä¸Šä¸Šä¼ äº†loraå¾®è°ƒèåˆåçš„Llama-3-8B-Chinese-chat-v0.1[unstoppable123/Llama-3-8B-Chinese-chat-v0.1](https://huggingface.co/unstoppable123/Llama-3-8B-Chinese-chat-v0.1)ä»¥åŠLlama3-8B-Chinese-Chat-lora-v0.1çš„loraæƒé‡[unstoppable123/Llama-3-8B-chinese-lora-v0.1](https://huggingface.co/unstoppable123/Llama-3-8B-chinese-lora-v0.1)!

## âš¡ï¸ å¿«é€Ÿå¼€å§‹

- **è¿è¡ŒLlama-3-8B-Chinese-chat**


```python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"     #before torch
from transformers import AutoTokenizer, AutoConfig, AddedToken, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from dataclasses import dataclass
from typing import Dict
import torch
import copy

## template
@dataclass
class Template:
    template_name:str
    system_format: str
    user_format: str
    assistant_format: str
    system: str
    stop_word: str

template_dict: Dict[str, Template] = dict()

def register_template(template_name, system_format, user_format, assistant_format, system, stop_word=None):
    template_dict[template_name] = Template(
        template_name=template_name,
        system_format=system_format,
        user_format=user_format,
        assistant_format=assistant_format,
        system=system,
        stop_word=stop_word,
    )

# not match with my version
register_template(
    template_name='llama3',
    # system_format='<|begin_of_text|><<SYS>>\n{content}\n<</SYS>>\n\n',     #previous
    system_format="<|start_header_id|>system<|end_header_id|>\n\n{content}<|eot_id|>",   #same
    user_format='<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>',  #same
    # assistant_format='<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|end_of_text|>\n',    #previous
    assistant_format='<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|eot_id|>\n',
    system="You are a helpful assistant. ",  #same
    stop_word='<|eot_id|>'    #same
)



def load_model(model_name_or_path, adapter_name_or_path=None):

    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,
        # low_cpu_mem_usage=True,
        torch_dtype=torch.float16,
        device_map='auto'
    )

    
    if adapter_name_or_path is not None:
        model = PeftModel.from_pretrained(model, adapter_name_or_path)

    return model


def load_tokenizer(model_name_or_path):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,
        use_fast=False
    )
    # print("pad token:", tokenizer.pad_token)
    if tokenizer.pad_token is None:
        # print("set pad token")
        tokenizer.pad_token = tokenizer.eos_token

    return tokenizer


def build_prompt(tokenizer, template, query, history, system=None):
    template_name = template.template_name
    system_format = template.system_format   # '<|begin_of_text|><<SYS>>\n{content}\n<</SYS>>\n\n'
    user_format = template.user_format      # '<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>'
    assistant_format = template.assistant_format   #'<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|end_of_text|>\n'
    system = system if system is not None else template.system   # "You are a helpful assistant. "

    history.append({"role": 'user', 'message': query})
    input_ids = []

    
    if system_format is not None:
        if system is not None:
            system_text = system_format.format(content=system)
            input_ids = tokenizer.encode(system_text, add_special_tokens=False)
   
    for item in history:
        role, message = item['role'], item['message']
        if role == 'user':
            message = user_format.format(content=message, stop_token=tokenizer.eos_token)
        else:
            message = assistant_format.format(content=message, stop_token=tokenizer.eos_token)
        tokens = tokenizer.encode(message, add_special_tokens=False)
        input_ids += tokens
    input_ids = torch.tensor([input_ids], dtype=torch.long)

    return input_ids


def main():
    model_name_or_path = 'unstoppable123/Llama-3-8B-Chinese-chat-v0.1' # your path
    template_name = 'llama3'
    adapter_name_or_path = None

    template = template_dict[template_name]

    max_new_tokens = 256 
    top_p = 0.9
    temperature = 0.6 
    repetition_penalty = 1.1 

    
    print(f'Loading model from: {model_name_or_path}')
    print(f'adapter_name_or_path: {adapter_name_or_path}')
    model = load_model(
        model_name_or_path,
        adapter_name_or_path=adapter_name_or_path
    ).eval()
    tokenizer = load_tokenizer(model_name_or_path if adapter_name_or_path is None else adapter_name_or_path)
    if template.stop_word is None:
        template.stop_word = tokenizer.eos_token
    stop_token_id = tokenizer.encode(template.stop_word, add_special_tokens=True)
    assert len(stop_token_id) == 1
    stop_token_id = stop_token_id[0]

    history = []

    query = input('# Userï¼š')
    while True:
        query = query.strip()
        input_ids = build_prompt(tokenizer, template, query, copy.deepcopy(history), system=None).to(model.device)
        # print("tokenizer.pad_token_id", tokenizer.pad_token_id)
        outputs = model.generate(
            input_ids=input_ids, max_new_tokens=max_new_tokens, do_sample=True,
            top_p=top_p, temperature=temperature, repetition_penalty=repetition_penalty,
            eos_token_id=stop_token_id,
            pad_token_id=tokenizer.eos_token_id
        )
        outputs = outputs.tolist()[0][len(input_ids[0]):]
        response = tokenizer.decode(outputs)
        response = response.strip().replace(template.stop_word, "").strip()

        # history
        history.append({"role": 'user', 'message': query})
        history.append({"role": 'assistant', 'message': response})


        if len(history) > 12:
            history = history[:-12]
        if response.startswith("<|start_header_id|>assistant<|end_header_id|>"):
            response = response[len("<|start_header_id|>assistant<|end_header_id|>"):]
        if response.startswith("system<|end_header_id|>"):
            response = response[len("system<|end_header_id|>"):]
        print("# Llama3-Chineseï¼š{}".format(response))
        query = input('# Userï¼š')


if __name__ == '__main__':
    main()
```

## ğŸš€ å·¥ä½œæµ

## ğŸ¤— æ¨¡å‹
### ğŸ¤– Llama-3å®˜æ–¹æ¨¡å‹
|  ç±»åˆ«  | æ¨¡å‹åç§°        | ğŸ¤—æ¨¡å‹åŠ è½½åç§°                  | ä¸‹è½½åœ°å€                                                     |
| --------------- | --------------- | ------------------------------ | ------------------------------------------------------------ |
|  é¢„è®­ç»ƒ  | Llama-3-8B  | meta-llama/Meta-Llama-3-8B  | [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B) |
|  Chat  | Llama-3-8B-Instrcut  | meta-llama/Meta-Llama-3-8B-Instruct  | [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |

### ğŸ¤– ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹

### ğŸ¤– ä¸­æ–‡å¾®è°ƒæ¨¡å‹
|  ç±»åˆ«  | æ¨¡å‹åç§°        | ğŸ¤—æ¨¡å‹åŠ è½½åç§°                  | ä¸‹è½½åœ°å€                                                     |
| --------------- | --------------- | ------------------------------ | ------------------------------------------------------------ |
|  loraå¾®è°ƒ(å·²èåˆ)  | Llama-3-8B-Chinese-chat-v0.1  | unstoppable123/Llama-3-8B-Chinese-chat-v0.1  | [HuggingFace](https://huggingface.co/unstoppable123/Llama-3-8B-Chinese-chat-v0.1) |
|  loraå¾®è°ƒ(loraæƒé‡)  | Llama3-8B-Chinese-Chat-lora-v0.1  | unstoppable123/Llama-3-8B-chinese-lora-v0.1  | [HuggingFace](https://huggingface.co/unstoppable123/Llama-3-8B-chinese-lora-v0.1) |

### ğŸ¤– ä¸­æ–‡å¯¹é½æ¨¡å‹

## ğŸ“ æ•°æ®é›†
### ğŸ“„ é¢„è®­ç»ƒæ•°æ®é›†

### ğŸ“„ å¾®è°ƒæ•°æ®é›†
| åç§°                                                      | æ•°æ®é‡ |æè¿°                                                         |
| ---------------------------------------------------------- | --------- |------------------------------------------------------------ |
| [Firefly](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M) |1150k    | åŒ…å«23ç§å¸¸è§ä¸­æ–‡ä¸‹æ¸¸ä»»åŠ¡çš„ä¼˜è´¨ä¸­æ–‡æ•°æ®ï¼Œå¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œç”±äººå·¥ä¹¦å†™è‹¥å¹²ç§æŒ‡ä»¤æ¨¡æ¿ï¼Œä¿è¯æ•°æ®çš„é«˜è´¨é‡ä¸ä¸°å¯Œåº¦   |
| [Ruozhiba](https://huggingface.co/datasets/LooksJuicy/ruozhiba)  |15k  | å¼±æ™ºå§æ•°æ®é—®ç­”ï¼Œæœ‰åŠ©äºå¢å¼ºæ¨¡å‹å›ç­”çš„é€»è¾‘æ€§ |
| [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) |50k  | å¾®è½¯å¼€æºçš„æ•°æ®ï¼Œæ˜¯åˆ©ç”¨ GPT-4 ç”Ÿæˆçš„ Alpaca æ•°æ®ï¼Œå¹¶åšäº†ä¸­æ–‡ç¿»è¯‘ |
| [COIG](https://huggingface.co/datasets/BAAI/COIG) |160k | åŒ…å«çŸ¥ä¹ã€å°çº¢ä¹¦å†å²é—®ç­”ã€ä»·å€¼è§‚ã€ä»£ç é—®é¢˜è§£å†³ç­‰è€ƒé¢˜å‹æ•°æ® |

### ğŸ“„ å¯¹é½æ•°æ®é›†

## ğŸ”§ è®¾ç½®

## ğŸ“– æ ·ä¾‹
- v2å›ç­”æ ·ä¾‹ï¼ˆUseréƒ¨åˆ†ä¸ºç”¨æˆ·è¾“å…¥ï¼ŒAssistantéƒ¨åˆ†ä¸ºæ¨¡å‹ç”Ÿæˆå›å¤ï¼‰
''' bash
User: è§£é‡Šä¸€ä¸‹VUEåŸç†

Assistant: VUEæ˜¯ä¸€ç§åŸºäºå“åº”å¼ç¼–ç¨‹çš„å‰ç«¯æ¡†æ¶ï¼Œä½¿ç”¨äº†è§‚å¯Ÿè€…æ¨¡å¼æ¥å®ç°æ•°æ®çš„åŒå‘ç»‘å®šã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ä¸€ä¸ªå¯¹è±¡æ¥å­˜å‚¨çŠ¶æ€ï¼Œå¹¶å°†å…¶è§‚å¯Ÿè€…æ¨¡å¼çš„è§‚å¯Ÿè€…åˆ—è¡¨ä¿å­˜åœ¨å¯¹è±¡çš„å±æ€§ä¸­ã€‚è¿™æ ·ï¼Œå½“å¯¹è±¡çš„çŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶ï¼Œè§‚å¯Ÿè€…åˆ—è¡¨ä¸­çš„è§‚å¯Ÿè€…å¯¹è±¡å°†è¢«é€šçŸ¥å¹¶æ›´æ–°ç›¸åº”çš„è§†å›¾ã€‚VUEçš„å®ç°ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

1. å“åº”å¼æ•°æ®æºï¼šVUEä½¿ç”¨äº†å“åº”å¼æ•°æ®æºæ¥å­˜å‚¨çŠ¶æ€ï¼Œå“åº”å¼æ•°æ®æºçš„å€¼ä¼šè¢«å®æ—¶ç›‘æµ‹ï¼Œå¹¶ä¸”ä¼šé€šçŸ¥è§‚å¯Ÿè€…å¯¹è±¡è¿›è¡Œç›¸åº”çš„æ›´æ–°ã€‚

2. è§‚å¯Ÿè€…æ¨¡å¼ï¼šVUEä½¿ç”¨äº†è§‚å¯Ÿè€…æ¨¡å¼æ¥å®ç°æ•°æ®çš„åŒå‘ç»‘å®šã€‚è§‚å¯Ÿè€…å¯¹è±¡ä¼šè¢«é€šçŸ¥å½“å“åº”å¼æ•°æ®æºçš„å€¼å‘ç”Ÿå˜åŒ–æ—¶ï¼Œè§‚å¯Ÿè€…å¯¹è±¡ä¼šè¿›è¡Œç›¸åº”çš„æ›´æ–°ã€‚

3. æ•°æ®ç»‘å®šï¼šVUEä½¿ç”¨äº†æ•°æ®ç»‘å®šæ¥å°†å“åº”å¼æ•°æ®æºçš„å€¼ä¸è§†å›¾è¿›è¡Œç»‘å®šã€‚æ•°æ®ç»‘å®šä¼šå°†å“åº”å¼æ•°æ®æºçš„å€¼ä¸è§†å›¾ä¸­çš„å…ƒç´ è¿›è¡Œç»‘å®šï¼Œå¹¶ä¸”å½“å“åº”å¼æ•°æ®æºçš„å€¼å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ•°æ®ç»‘å®šä¼šè‡ªåŠ¨æ›´æ–°è§†å›¾ä¸­çš„å…ƒç´ ã€‚

4. ä¾èµ–é¡¹ç®¡ç†ï¼šVUEä½¿ç”¨äº†ä¾èµ–é¡¹ç®¡ç†æ¥å®ç°å“åº”å¼æ•°æ®æºçš„æ›´æ–°ã€‚å½“å“åº”å¼æ•°æ®æºçš„å€¼å‘ç”Ÿå˜åŒ–æ—¶ï¼ŒVUEä¼šæ ¹æ®ä¾èµ–é¡¹ç®¡ç†çš„è§„åˆ™æ¥æ›´æ–°å“åº”å¼æ•°æ®æºçš„å€¼ï¼Œå¹¶é€šçŸ¥è§‚å¯Ÿè€…å¯¹è±¡è¿›è¡Œç›¸åº”çš„æ›´æ–°ã€‚

5. ç»„ä»¶åŒ–ï¼šVUEä½¿ç”¨äº†ç»„ä»¶åŒ–çš„æ€æƒ³æ¥å®ç°å‰ç«¯å¼€å‘ã€‚ç»„ä»¶åŒ–å°†åº”ç”¨ç¨‹åºåˆ†æˆå¤šä¸ªå°çš„ã€ç‹¬ç«‹çš„æ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—éƒ½å¯ä»¥å•ç‹¬åœ°å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²ã€‚ç»„ä»¶åŒ–ä½¿å¾—å‰ç«¯å¼€å‘æ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚

æ€»ä¹‹ï¼ŒVUEçš„å®ç°ä¸»è¦åŸºäºå“åº”å¼ç¼–ç¨‹çš„æ€æƒ³ï¼Œä½¿ç”¨äº†è§‚å¯Ÿè€…æ¨¡å¼å’Œæ•°æ®ç»‘å®šæ¥å®ç°æ•°æ®çš„åŒå‘ç»‘å®šï¼Œå¹¶ä½¿ç”¨ä¾èµ–é¡¹ç®¡ç†å’Œç»„ä»¶åŒ–çš„æ€æƒ³æ¥å®ç°å‰ç«¯å¼€å‘ã€‚

'''
